# ğŸ›¡ï¸ Phishing URL Detection using Machine Learning  

Building a robust classification model to proactively identify and neutralize malicious phishing URLs using textual and domain-based features.  

---

## ğŸ“Œ Project Overview  
This project focuses on developing a **machine learning classifier** to accurately detect phishing URLs.  
Leveraging the **PhiUSIIL Phishing URL Dataset**, we preprocess a rich set of featuresâ€”from URL length to domain characteristicsâ€”to create a model that can serve as a critical component of a web security system.  

**Objective:** Build a reliable, automated tool that identifies online threats in real-time.  

---

## ğŸ¯ Objectives  
- Preprocess the real-world PhiUSIIL dataset  
- Drop irrelevant and high-cardinality string features  
- Handle high-cardinality categorical features such as TLD efficiently  
- Apply one-hot encoding and feature scaling  
- Split the data using a stratified train/test split  
- Prepare a clean dataset for machine learning modeling  

---

## ğŸ“ Dataset  
- **Source:** [UCI Machine Learning Repository](https://archive.ics.uci.edu/)  
- **Name:** PhiUSIIL Phishing URL Dataset  
- **Task:** Binary Classification  
  - `0 = Phishing`  
  - `1 = Legitimate`  

---

## ğŸ”§ Stage 1: Data Collection & Exploration  
- âœ… Loaded data using `ucimlrepo`  
- âœ… Inspected structure, columns, data types, and label distribution  
- âœ… Evaluated all 52 features for relevance  

---

## ğŸ§¼ Stage 2: Preprocessing & Feature Engineering  

### Dropping Irrelevant Columns  
- `FILENAME` â€“ File identifier  
- `URL`, `Domain` â€“ High-cardinality strings (info already captured in engineered features)  
- `Title` â€“ Unstructured text  

### Handling High-Cardinality Feature: TLD  
- 695 unique values  
- Grouped TLDs with frequency `< 10` as `"Other"`  
- Result: Reduced dimensionality while preserving useful signal  

### Encoding and Scaling  
- One-hot encoding applied to categorical feature (TLD)  
- StandardScaler applied to numerical features  
- Train/Test split: 80/20 with stratify=y  

---

## ğŸš€ Stage 3: Model Building  
Trained multiple classifiers:  
- Logistic Regression  
- Decision Tree Classifier  
- Random Forest Classifier  
- Support Vector Machine (SVM)  
- K-Nearest Neighbors (k-NN)  
- Gradient Boosting Classifier  

---

## âš™ï¸ Stage 4: Hyperparameter Tuning  
- Tool: `RandomizedSearchCV`  
- Best Model: **Random Forest**  
- Best Accuracy: **0.9999**  

---

## ğŸ“Š Stage 5: Evaluation Metrics  
- Accuracy  
- Classification Report  
- Confusion Matrix  

---

## ğŸ“ˆ Stage 6: Model Performance Results  

| Model                        | Accuracy   |
|------------------------------|------------|
| ğŸŒ³ Random Forest Classifier  | **0.999915** |
| ğŸŒ² Decision Tree Classifier  | 0.999894   |
| â— Logistic Regression       | 0.999576   |
| ğŸ“ K-Nearest Neighbors       | 0.997010   |
| âš–ï¸ Support Vector Machine    | 0.997010   |

### Final Model: Random Forest  

**Accuracy:** 0.999915  

**Classification Report:**  

          precision    recall  f1-score   support
       0       1.00      1.00      1.00     23464
       1       1.00      1.00      1.00     23464

accuracy                           1.00     46928



**Confusion Matrix:**  

|               | Predicted Phishing | Predicted Legitimate |
|---------------|--------------------|----------------------|
| Actual Phishing   | âœ… 23464 | âŒ 0   |
| Actual Legitimate | âŒ 4     | âœ… 23460 |

---

## ğŸ† Stage 7: Conclusion  
- The **Random Forest Classifier** achieved the highest accuracy.  
- Key strengths:  
  - Handles numerical + categorical features effectively  
  - Robust against overfitting  
  - Feature importance for interpretability  

âœ… Final model saved as `random_forest_model.pkl` for deployment in a real-time threat detection pipeline.  

---

## ğŸ› ï¸ Tech Stack  
- **Language:** Python  
- **Libraries:** pandas, numpy, scikit-learn, matplotlib, seaborn  
- **Tools:** Jupyter Notebook  

---

 
